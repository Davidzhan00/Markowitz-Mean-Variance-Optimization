{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from arch import arch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download: SP500 and stocks price data and convert into logarithmic daily returns, and 1-year US governmnet bond yields and convert to daily yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biggest public companies by market-cap as of 01/01/2010\n",
    "stocks = [\"XOM\", \"MSFT\", \"WMT\", \"GOOG\", \"AAPL\", \"JNJ\", \"PG\", \"JPM\", \"IBM\", \"CVX\", \"BRK-B\", \"WFC\", \"PFE\", \"CSCO\", \"KO\", \"BAC\", \"GE\", \"T\", \"ORCL\", \"INTC\"]\n",
    "\n",
    "# Downloading data from 1y earlier - results will not consider 2009\n",
    "data = yf.download(stocks, start=\"2009-01-01\", end=\"2023-12-31\")['Adj Close']\n",
    "data = data.dropna()\n",
    "\n",
    "log_returns = np.log(data / data.shift(1)).dropna()\n",
    "log_returns_demean = log_returns.subtract(log_returns.mean(axis=0), axis=1)\n",
    "\n",
    "sp500 = yf.download([\"^GSPC\"], start=\"2010-01-01\", end=\"2023-12-31\")['Adj Close']\n",
    "sp500_returns = np.log(sp500 / sp500.shift(1)).dropna()\n",
    "sp500_norm = 100*sp500/sp500.iloc[0]\n",
    "\n",
    "risk_free = yf.download('^IRX', start=\"2009-01-01\", end=\"2023-12-31\")\n",
    "risk_free = risk_free[['Close']].rename(columns={'Close': 'Daily Yield'})\n",
    "risk_free = (1 + risk_free / 100) ** (1 / 365) - 1\n",
    "\n",
    "log_returns_demean.index = log_returns_demean.index.date\n",
    "risk_free.index = risk_free.index.date\n",
    "sp500_returns.index = sp500_returns.index.date\n",
    "sp500_norm.index = sp500_norm.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create three estimates of daily mean returns: historical mean, moving average, and exponentially-weighted moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unconditional Historical Means\n",
    "gross_mean = pd.DataFrame(index = log_returns_demean.index, columns = log_returns.columns)\n",
    "for n in range(1, (len(log_returns)-25)):\n",
    "    mean_returns.iloc[25+n] = log_returns[:(24+n)].mean(axis=0)\n",
    "gross_mean = mean_returns.dropna()\n",
    "\n",
    "# 25-Trading Day Window Means\n",
    "moving_average = pd.DataFrame(index = log_returns_demean.index, columns = log_returns.columns)\n",
    "for n in range(1, (len(log_returns)-25)):\n",
    "    mean_returns.iloc[25+n] = log_returns[(n-1):(24+n)].mean(axis=0)\n",
    "moving_average = mean_returns.dropna()\n",
    "\n",
    "# Exponentially-Weighted Moving Averages\n",
    "ewma = pd.DataFrame(0.00, index = log_returns_demean.index, columns = log_returns.columns)\n",
    "for n in range(1, (len(log_returns)-25)):\n",
    "    lambda_ewma = 0.5\n",
    "    w_1 = (1-lambda_ewma)/(1-lambda_ewma**(24+n))\n",
    "    for i in range(1, 25+n):\n",
    "        ewma.iloc[25+n] += log_returns.iloc[25+n-i] * w_1 * lambda_ewma ** (i-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to generate the principal components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_rotation(data): # With a TxN dataframe (e.g. log_returns), it produces the NxN rotation matrix\n",
    "    \n",
    "    length = len(data)\n",
    "    coln = len(stocks)\n",
    "    pca = PCA(n_components = coln)\n",
    "    principal_components = pca.fit(data)\n",
    "    rotation_matrix = pca.components_\n",
    "    column_names = [f'PC{i+1}' for i in range(coln)]\n",
    "    df_rotation = pd.DataFrame(rotation_matrix, columns = column_names, index = stocks)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    df_rotation.loc['Explained Variance'] = explained_variance\n",
    "    \n",
    "    return df_rotation\n",
    "\n",
    "def pca_fit(data): # With a TxN dataframe (e.g. log_returns), it produces a TxN dataframe with the PCA beta estimates \n",
    "    \n",
    "    length = len(data)\n",
    "    coln = len(stocks)\n",
    "    pca = PCA(n_components=coln)\n",
    "    column_names = [f'PC{i + 1}' for i in range(coln)]\n",
    "    principal_components = pca.fit_transform(data)\n",
    "    df_fit = pd.DataFrame(principal_components, columns=column_names, index=data.index)\n",
    "\n",
    "    return df_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the univariate GARCH(1,1) estimation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni_garch(data): # With a Tx1 vector (e.g. log_returns), it produces the Tx1 dataframe with univariate GARCH(1,1) estimates of conditional variance\n",
    "    \n",
    "    model = arch_model(data*100, mean = 'Zero', vol='Garch', p=1, q=1)\n",
    "    garch_fit = model.fit(disp='warn')\n",
    "    variance = (garch_fit.conditional_volatility ** 2) / (100**2)\n",
    "\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_covar(data): # With a TxN dataframe, it produces the conditional variance-covariance matrix for date T+1\n",
    "\n",
    "    rotation = pca_rotation(data)\n",
    "    fit = pca_fit(data)\n",
    "    columns_to_drop = []  # Using only significant factors\n",
    "\n",
    "    for i in range(1, len(stocks) + 1):\n",
    "        if rotation.loc['Explained Variance', f'PC{i}'] < 1/(2 * len(stocks)):\n",
    "            columns_to_drop.append(f'PC{i}')\n",
    "    rotation_significant = rotation.drop(columns=columns_to_drop)\n",
    "    rotation_significant = rotation_significant.iloc[:len(stocks)]\n",
    "    fit_significant = fit.drop(columns=columns_to_drop)\n",
    "    significant_factors = fit_significant.columns\n",
    "\n",
    "    errors = data - fit_significant.dot(rotation_significant.T)  # Finding error variances\n",
    "    var_errors = pd.DataFrame(columns=stocks, index=data.index)\n",
    "    for s in stocks:\n",
    "        var_errors.loc[:, s] = uni_garch(errors.loc[:, s])\n",
    "\n",
    "    var_factors = pd.DataFrame(columns=significant_factors, index=data.index)  # Finding factor variances\n",
    "    for f in significant_factors:\n",
    "        var_factors[f] = uni_garch(fit_significant[f])\n",
    "\n",
    "    rotation_values = rotation_significant.values # Computing variance-covariance matrix for time t\n",
    "    sigma_factors_diag = np.diag(var_factors.iloc[-1])\n",
    "    var_covar_data = rotation_values.dot(sigma_factors_diag).dot(rotation_values.T) + np.diag(var_errors.iloc[-1])\n",
    "    var_covar_t = pd.DataFrame(var_covar_data, columns = stocks, index = stocks)\n",
    "\n",
    "    return var_covar_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to generate portfolio mean returns and conditional volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_return(t, portfolio, mean_returns): # Given date t, a vector portfolio defining the weights for each stock, and the mean returns estimates, it produces the portfolio's mean return\n",
    "    \n",
    "    weights = portfolio\n",
    "    returns = mean_returns.loc[t]\n",
    "    mean_return = weights.dot(returns.T)\n",
    "\n",
    "    return mean_return\n",
    "\n",
    "def portfolio_vol(sigma, portfolio): # Given a variance-covariance matrix (for date t) and a vector portfolio defining the weights for each stock, it produces the portfolio's conditional volatility\n",
    "    \n",
    "    weights = portfolio\n",
    "    var = (weights).dot(sigma).dot(weights.T)\n",
    "    vol = var ** (1/2)\n",
    "    \n",
    "    return vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to run the Markowitz mean-variance optimization given some data inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_portfolio(data, risk_free, mean_returns): # Given a TxN dataframe (e.g. log_returns) and a 1x1 dataframe containing the daily risk-free rate for date T, it finds mean-variance optimal portfolio by simulation\n",
    "\n",
    "    weights = np.random.uniform(0, 10, (10000, len(stocks))) # Generate 10,000 simulated portfolios\n",
    "    weights /= weights.sum(axis=1)[:, None] # Standarize the weights such that the sum of all weights equals 1; individual weights are downwardly and upwardly bounded by 0 and 1\n",
    "    portfolio_r_vol = pd.DataFrame(columns = [\"Return\", \"Volatility\", \"Slope\"], index = range(10000))\n",
    "    \n",
    "    sigma = var_covar(data)\n",
    "    rf = risk_free.values\n",
    "    t = data.index[-1]\n",
    "    \n",
    "    for i in range(10000):\n",
    "        portfolio_r_vol.loc[i, \"Return\"] = portfolio_return(t, weights[i], mean_returns)\n",
    "        portfolio_r_vol.loc[i, \"Volatility\"] = portfolio_vol(sigma, weights[i])\n",
    "        portfolio_r_vol.loc[i, \"Slope\"] = (portfolio_r_vol.loc[i, \"Return\"] - rf) / portfolio_r_vol.loc[i, \"Volatility\"]\n",
    "    optimal_weights = weights[portfolio_r_vol['Slope'].idxmax()]\n",
    "    \n",
    "    return optimal_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run algorithm using all three different estimates of mean returns and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_gross_mean = pd.DataFrame(columns = stocks, index = log_returns_demean.index)\n",
    "algo_moving_average = pd.DataFrame(columns = stocks, index = log_returns_demean.index)\n",
    "algo_ewma = pd.DataFrame(columns = stocks, index = log_returns_demean.index)\n",
    "\n",
    "for t in log_returns_demean.index:\n",
    "    print(f\"Finding optimal portfolio for {t}\")\n",
    "    try:\n",
    "        data = log_returns_demean[:t]\n",
    "        rf = risk_free.loc[t]\n",
    "        algo_gross_mean.loc[t] = optimal_portfolio(data, rf, gross_mean[:t])\n",
    "        algo_moving_average.loc[t] = optimal_portfolio(data, rf, moving_average[:t])\n",
    "        algo_ewma.loc[t] = optimal_portfolio(data, rf, ewma[:t])\n",
    "    except Exception as e:\n",
    "        print(f\"Risk-free rate not found for date {t}. Skipping this date.\")\n",
    "        continue\n",
    "\n",
    "with open('algo_portfolio.pkl', 'wb') as f:\n",
    "    pickle.dump(algo_gross_mean, f)\n",
    "\n",
    "with open('algo_portfolio1.pkl', 'wb') as f:\n",
    "    pickle.dump(algo_moving_average, f)\n",
    "\n",
    "with open('algo_portfolio2.pkl', 'wb') as f:\n",
    "    pickle.dump(algo_ewma, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and use only from 2010-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('algo_portfolio.pkl', 'rb') as f:\n",
    "    gross_mean = pickle.load(f).dropna()\n",
    "\n",
    "with open('algo_portfolio1.pkl', 'rb') as f:\n",
    "    moving_average = pickle.load(f).dropna()\n",
    "\n",
    "with open('algo_portfolio2.pkl', 'rb') as f:\n",
    "    ewma = pickle.load(f).dropna()\n",
    "\n",
    "algo_weights = [gross_mean, moving_average, ewma]\n",
    "for x in algo_weights:\n",
    "    x = x[pd.to_datetime(x.index) >= pd.to_datetime(\"2010-01-01\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the returns of the algorithm-generated portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_mean_r = pd.DataFrame(columns = [\"Historical Mean\"], index = gross_mean.index)\n",
    "for t in gross_mean_r.index:\n",
    "    weights = gross_mean.loc[t].values\n",
    "    r = log_returns.loc[t].values\n",
    "    gross_mean_r.loc[t, \"Historical Mean\"] = r.dot(weights.T)\n",
    "\n",
    "moving_average_r = pd.DataFrame(columns = [\"Moving Average\"], index = moving_average.index)\n",
    "for t in moving_average_r.index:\n",
    "    weights = moving_average.loc[t].values\n",
    "    r = log_returns.loc[t].values\n",
    "    moving_average_r.loc[t, \"Moving Average\"] = r.dot(weights.T)\n",
    "\n",
    "ewma_r = pd.DataFrame(columns = [\"EWMA\"], index = ewma.index)\n",
    "for t in ewma_r.index:\n",
    "    weights = ewma.loc[t].values\n",
    "    r = log_returns.loc[t].values\n",
    "    ewma_r.loc[t, \"EWMA\"] = r.dot(weights.T)\n",
    "\n",
    "algo_portfolio = pd.concat([gross_mean_r, moving_average_r, ewma_r, sp500_returns], axis=1).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every day, rank the performance of the different portfolios and the SP500 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = algo_portfolio.rank(axis=1, method='dense', ascending=False).astype(int)\n",
    "rank = 5 - rank\n",
    "columns = [\"Historical Mean\", \"Moving Average\", \"EWMA\", \"SP500\"]\n",
    "rank.columns = [\"Historical Mean\", \"Moving Average\", \"EWMA\", \"SP500\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert daily returns into daily prices with base 2010-01-01 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_returns = [gross_mean_r, moving_average_r, ewma_r]\n",
    "algo_results = pd.DataFrame(columns = columns, index = algo_portfolio.index)\n",
    "algo_results.iloc[0, 0:3] = 100\n",
    "\n",
    "for i in range(1, len(algo_portfolio.index)):\n",
    "    algo_results.iloc[i, 0] = algo_results.iloc[i-1, 0] * math.e ** algo_portfolio.iloc[i, 0]\n",
    "    algo_results.iloc[i, 1] = algo_results.iloc[i-1, 1] * math.e ** algo_portfolio.iloc[i, 1]\n",
    "    algo_results.iloc[i, 2] = algo_results.iloc[i - 1, 2] * math.e ** algo_portfolio.iloc[i, 2]\n",
    "    algo_results.iloc[i, 3] = algo_results.iloc[i - 1, 3] * math.e ** algo_portfolio.iloc[i, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the value of the SP500 vs the different mean return estimates (e.g. EWMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sp500_norm.index, sp500_norm, label='SP500', color='black', linewidth = 0.8)\n",
    "for i in range(1, len(algo_results)):\n",
    "    if rank[\"SP500\"].iloc[i] > rank[\"EWMA\"].iloc[i]:\n",
    "        color = 'orange'  # SP500 rank is higher than EWMA rank\n",
    "    else:\n",
    "        color = 'green'  # SP500 rank is lower than EWMA rank\n",
    "    plt.plot(algo_results.index[i - 1:i + 1], algo_results[\"EWMA\"].iloc[i - 1:i + 1], color=color, linewidth=0.8)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
